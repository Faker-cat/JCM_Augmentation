import os

import pandas as pd

# --- è¨­å®š ---
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã§å®šç¾©
EVAL_DIR = "data/02_prepared_for_eval"
OUTPUT_DIR = "data/03_merged_results"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "JCM_MERGED_EVALUATIONS.csv")

# ğŸ’¡ è©•ä¾¡è€…ãŒå¢—ãˆãŸã‚‰ã€ã“ã®ãƒªã‚¹ãƒˆã«åå‰ã‚’è¿½åŠ 
EVALUATORS = ["A", "B", "C"]
TOTAL_EVALUATORS = len(EVALUATORS)
MAJORITY_THRESHOLD = (TOTAL_EVALUATORS // 2) + 1  # éåŠæ•° (ä¾‹: 3äººä¸­ 2äºº, 5äººä¸­ 3äºº)

# çµ±åˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ãƒ«ãƒ‘ã‚¹ã§æ§‹æˆ
EVALUATION_FILES = [
    os.path.join(EVAL_DIR, f"{evaluator}_evaluation.csv") for evaluator in EVALUATORS
]

ID_COLUMN = "ID"
SENTENCE_COLUMN = "sent"
# ------------


def safe_merge_evaluations(file_list, output_file, id_col, sent_col, evaluators):
    print("1. è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆã‚’é–‹å§‹ã—ã¾ã™...")

    base_df = None

    # å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’IDã‚’ã‚­ãƒ¼ã«çµåˆã—ã¦ã„ã
    for i, file_name in enumerate(file_list):
        if not os.path.exists(file_name):
            print(
                f"è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            continue

        try:
            # ğŸ’¡ Google Sheetsã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸCSVã¯ä½™åˆ†ãªæ”¹è¡Œã‚³ãƒ¼ãƒ‰ã‚„ãƒã‚¤ãƒˆåˆ—ã‚’å«ã‚€å ´åˆãŒã‚ã‚‹ãŸã‚ã€low_memory=False
            df_new = pd.read_csv(file_name, encoding="utf-8", low_memory=False)

            # ğŸ’¡ ä¿®æ­£ç‚¹1: èª­ã¿è¾¼ã¿ç›´å¾Œã«ã€IDãŒæ¬ æã—ã¦ã„ã‚‹è¡Œï¼ˆç©ºè¡Œãªã©ï¼‰ã‚’å‰Šé™¤
            if id_col in df_new.columns:
                initial_len = len(df_new)
                df_new = df_new.dropna(subset=[id_col])
                dropped_len = initial_len - len(df_new)
                if dropped_len > 0:
                    print(
                        f"   (æƒ…å ±: '{file_name}' ã‹ã‚‰IDãŒç©ºã®è¡Œã‚’ {dropped_len} ä»¶å‰Šé™¤ã—ã¾ã—ãŸ)"
                    )

        except Exception as e:
            print(
                f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­ã¿è¾¼ã¿ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}"
            )
            continue

        # è©•ä¾¡è€…ã®ãƒ•ãƒ©ã‚°åˆ—ã‚’ç‰¹å®š
        flag_col = [
            col for col in df_new.columns if "ã®ãƒ•ãƒ©ã‚°" in col and col != f"{id_col}_x"
        ]

        if not flag_col:
            print(
                f"è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã«ãƒ•ãƒ©ã‚°åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            continue

        # ä¿æŒã™ã‚‹åˆ—ã‚’é¸æŠ: ID, æ–‡ç« (æœ€åˆã®ã¿), ãƒ•ãƒ©ã‚°åˆ—
        cols_to_keep = [id_col] + flag_col

        if base_df is None:
            # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€IDã¨æ–‡ç« åˆ—ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
            if sent_col not in df_new.columns:
                print(
                    f"ã‚¨ãƒ©ãƒ¼: ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã«æ–‡ç« åˆ— '{sent_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚"
                )
                return
            cols_to_keep.append(sent_col)

            # Original_ID ãŒã‚ã‚Œã°ä¿æŒãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹
            if "Original_ID" in df_new.columns:
                cols_to_keep.append("Original_ID")

            base_df = df_new[cols_to_keep].copy()
            print(f"   -> ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ '{file_name}' ã‚’è¨­å®šã—ã¾ã—ãŸã€‚")

        else:
            # 2ã¤ç›®ä»¥é™ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€IDã‚’ã‚­ãƒ¼ã«ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«çµåˆ
            df_to_merge = df_new[cols_to_keep].copy()
            base_df = pd.merge(base_df, df_to_merge, on=id_col, how="left")
            print(f"   -> '{file_name}' ã®è©•ä¾¡çµæœã‚’çµ±åˆã—ã¾ã—ãŸã€‚")

    if base_df is None:
        print("ã‚¨ãƒ©ãƒ¼: çµ±åˆã§ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # --- 2. åˆ†æé …ç›®ã®è¿½åŠ  (æœ€ã‚‚é‡è¦ãªå‡¦ç†) ---
    print("\n2. åˆ†æé …ç›®ï¼ˆTRUEåˆ¤å®šæ•°ã€éåŠæ•°ãƒ•ãƒ©ã‚°ãªã©ï¼‰ã‚’è¨ˆç®—ã—ã¾ã™...")

    # ãƒ•ãƒ©ã‚°åˆ—åã‹ã‚‰ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ '_' ã‚’å‰Šé™¤
    flag_columns = [f"{evaluator}ã®ãƒ•ãƒ©ã‚°" for evaluator in evaluators]

    # å®Ÿéš›ã«base_dfã«å­˜åœ¨ã™ã‚‹ãƒ•ãƒ©ã‚°åˆ—åã®ã¿ã‚’æŠ½å‡ºã—ã¾ã™
    existing_flag_columns = [
        col
        for col in base_df.columns
        if "ã®ãƒ•ãƒ©ã‚°" in col and col.replace("_x", "").replace("_y", "") in flag_columns
    ]

    if len(existing_flag_columns) != TOTAL_EVALUATORS:
        print(
            f"è­¦å‘Š: çµ±åˆã•ã‚ŒãŸåˆ—æ•° ({len(existing_flag_columns)}) ãŒè©•ä¾¡è€…æ•° ({TOTAL_EVALUATORS}) ã¨ä¸€è‡´ã—ã¾ã›ã‚“ã€‚"
        )
        print("åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ç¶šè¡Œã—ã¾ã™ãŒã€çµæœãŒä¸æ­£ç¢ºãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    # TRUE/FALSEæ–‡å­—åˆ—ã‚’æ•°å€¤ (1/0) ã«å¤‰æ›
    for col in existing_flag_columns:
        # ã¾ãšæ¬ æå€¤ã‚’æ–‡å­—åˆ—ã®"FALSE"ç­‰ã¨ã—ã¦åŸ‹ã‚ã‚‹ã‹ã€å¤‰æ›æ™‚ã«å‡¦ç†ã™ã‚‹
        # ã“ã“ã§ã¯NaNãŒã‚ã£ã¦ã‚‚astype(str)ã§"nan"ã«ãªã‚Šã€æ¬¡ã®lambdaã§0ã«ãªã‚‹ã®ã§å®‰å…¨
        base_df[col] = base_df[col].astype(str).str.upper().str.strip()
        base_df[col] = base_df[col].apply(lambda x: 1 if x == "TRUE" else 0)

    # 1. æ›–æ˜§åˆ¤å®šæ•° (TRUEã¨åˆ¤æ–­ã—ãŸè©•ä¾¡è€…ã®åˆè¨ˆäººæ•°) ã‚’è¨ˆç®—
    base_df["TRUEåˆ¤å®šæ•°"] = base_df[existing_flag_columns].sum(axis=1)

    # 2. è©•ä¾¡è€…æ•°
    base_df["è©•ä¾¡è€…æ•°"] = TOTAL_EVALUATORS

    # 3. TRUEåˆ¤å®šå‰²åˆ (åˆæ„ç‡ã®æŒ‡æ¨™)
    base_df["TRUEåˆ¤å®šå‰²åˆ"] = base_df["TRUEåˆ¤å®šæ•°"] / base_df["è©•ä¾¡è€…æ•°"]

    # 4. éåŠæ•°TRUEãƒ•ãƒ©ã‚°
    base_df["éåŠæ•°TRUEãƒ•ãƒ©ã‚°"] = (base_df["TRUEåˆ¤å®šæ•°"] >= MAJORITY_THRESHOLD).astype(
        int
    )

    # 5. æœ€ä½1äººTRUEãƒ•ãƒ©ã‚°
    base_df["æœ€ä½1äººTRUEãƒ•ãƒ©ã‚°"] = (base_df["TRUEåˆ¤å®šæ•°"] >= 1).astype(int)

    print(f"   -> åˆ¤å®šæ•°ã®è¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚éåŠæ•°ã—ãã„å€¤: {MAJORITY_THRESHOLD}äººã€‚")

    # --- 3. æœ€çµ‚åˆ—é †ã®æ•´ç†ã¨ä¿å­˜ ---

    # Original_ID ã§ä¸¦ã³é †ã‚’å…ƒã«æˆ»ã™
    if "Original_ID" in base_df.columns:
        # ã‚½ãƒ¼ãƒˆå‰ã«Original_IDã®æ¬ æè¡Œã‚’å‰Šé™¤
        base_df = base_df.dropna(subset=["Original_ID"])
        base_df = base_df.sort_values("Original_ID")
        print("   -> 'Original_ID' ã«åŸºã¥ã„ã¦ä¸¦ã³é †ã‚’å…ƒã«æˆ»ã—ã¾ã—ãŸã€‚")

    # ğŸ’¡ ä¿®æ­£ç‚¹2: æœ€çµ‚çš„ãªIDåˆ—ã®æ¬ æè¡Œã‚’ç¢ºå®Ÿã«å‰Šé™¤
    base_df = base_df.dropna(subset=[id_col])

    # æ•´æ•°ã§ã‚ã‚‹ã¹ãåˆ—ã‚’æ˜ç¤ºçš„ã«intå‹ã«å¤‰æ›
    int_columns = [
        id_col,
        "Original_ID",
        "TRUEåˆ¤å®šæ•°",
        "è©•ä¾¡è€…æ•°",
        "éåŠæ•°TRUEãƒ•ãƒ©ã‚°",
        "æœ€ä½1äººTRUEãƒ•ãƒ©ã‚°",
    ]
    # ãƒ•ãƒ©ã‚°åˆ—ã‚‚è¿½åŠ 
    int_columns.extend(existing_flag_columns)

    for col in int_columns:
        if col in base_df.columns:
            try:
                # æ¬ æå€¤ãŒãªã„çŠ¶æ…‹ã«ãªã£ãŸã®ã§ã€å®‰å…¨ã«intå¤‰æ›ã§ãã‚‹ã¯ãš
                base_df[col] = base_df[col].astype(int)
            except ValueError as e:
                print(f"æ³¨æ„: åˆ— '{col}' ã®æ•´æ•°å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                pass

    # ç†æƒ³çš„ãªåˆ—é †ã‚’å®šç¾©
    primary_cols = [id_col, "Original_ID", sent_col]
    eval_flag_cols = existing_flag_columns
    analysis_cols = [
        "TRUEåˆ¤å®šæ•°",
        "TRUEåˆ¤å®šå‰²åˆ",
        "è©•ä¾¡è€…æ•°",
        "éåŠæ•°TRUEãƒ•ãƒ©ã‚°",
        "æœ€ä½1äººTRUEãƒ•ãƒ©ã‚°",
    ]

    final_cols = primary_cols + eval_flag_cols + analysis_cols
    final_cols_safe = [col for col in final_cols if col in base_df.columns]
    base_df = base_df[final_cols_safe]

    # ä¿å­˜
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    base_df.to_csv(output_file, index=False, encoding="utf-8")

    print(
        f"\nâœ… å®Œäº†: çµ±åˆãŠã‚ˆã³åˆ†æé …ç›®ãŒè¿½åŠ ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« '{output_file}' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚"
    )


# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
safe_merge_evaluations(
    EVALUATION_FILES, OUTPUT_FILE, ID_COLUMN, SENTENCE_COLUMN, EVALUATORS
)
