nohup: ignoring input
Loading data from /home/faker/JCM_Augmentation/data/00_raw/data_test.csv...
Using column 'sent' for input text.
Initializing model: Qwen/Qwen3-Next-80B-A3B-Instruct with TP=4...
INFO 12-05 17:56:41 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'max_model_len': 2048, 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.93, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-Next-80B-A3B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
WARNING 12-05 17:56:42 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
INFO 12-05 17:56:59 [model.py:637] Resolved architecture: Qwen3NextForCausalLM
INFO 12-05 17:56:59 [model.py:1750] Using max model len 2048
INFO 12-05 17:56:59 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-05 17:56:59 [config.py:315] Disabling cascade attention since it is not supported for hybrid models.
INFO 12-05 17:57:01 [config.py:439] Setting attention block size to 272 tokens to ensure that attention page size is >= mamba page size.
INFO 12-05 17:57:01 [config.py:463] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.
WARNING 12-05 17:57:01 [vllm.py:601] Enforce eager set, overriding optimization level to -O0
INFO 12-05 17:57:01 [vllm.py:707] Cudagraph is disabled under eager mode
WARNING 12-05 17:57:06 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[0;36m(EngineCore_DP0 pid=266633)[0;0m INFO 12-05 17:57:19 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen3-Next-80B-A3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen3-Next-80B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen3-Next-80B-A3B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
INFO 12-05 17:57:34 [parallel_state.py:1200] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:60927 backend=nccl
INFO 12-05 17:57:34 [parallel_state.py:1200] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:60927 backend=nccl
INFO 12-05 17:57:34 [parallel_state.py:1200] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:60927 backend=nccl
INFO 12-05 17:57:34 [parallel_state.py:1200] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:60927 backend=nccl
INFO 12-05 17:57:35 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 12-05 17:57:36 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 12-05 17:57:36 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 12-05 17:57:36 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 12-05 17:57:36 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
WARNING 12-05 17:57:36 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 12-05 17:57:36 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 12-05 17:57:36 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 12-05 17:57:36 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 12-05 17:57:36 [parallel_state.py:1408] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
INFO 12-05 17:57:36 [parallel_state.py:1408] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
INFO 12-05 17:57:36 [parallel_state.py:1408] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
INFO 12-05 17:57:36 [parallel_state.py:1408] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[0;36m(Worker_TP0 pid=266775)[0;0m INFO 12-05 17:57:37 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen3-Next-80B-A3B-Instruct...
[0;36m(Worker_TP3 pid=266778)[0;0m INFO 12-05 17:57:37 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(Worker_TP0 pid=266775)[0;0m INFO 12-05 17:57:38 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(Worker_TP1 pid=266776)[0;0m INFO 12-05 17:57:38 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(Worker_TP2 pid=266777)[0;0m INFO 12-05 17:57:38 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(Worker_TP3 pid=266778)[0;0m INFO 12-05 17:57:38 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP0 pid=266775)[0;0m INFO 12-05 17:57:38 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP1 pid=266776)[0;0m INFO 12-05 17:57:38 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP2 pid=266777)[0;0m INFO 12-05 17:57:38 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP1 pid=266776)[0;0m INFO 12-05 18:17:41 [weight_utils.py:487] Time spent downloading weights for Qwen/Qwen3-Next-80B-A3B-Instruct: 1202.757869 seconds
[0;36m(Worker_TP3 pid=266778)[0;0m INFO 12-05 18:17:43 [weight_utils.py:487] Time spent downloading weights for Qwen/Qwen3-Next-80B-A3B-Instruct: 0.551703 seconds
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:21<14:25, 21.63s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:42<13:39, 21.01s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:   7% Completed | 3/41 [01:01<12:44, 20.13s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  10% Completed | 4/41 [01:21<12:27, 20.20s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  12% Completed | 5/41 [01:42<12:21, 20.61s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  15% Completed | 6/41 [02:01<11:41, 20.03s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  17% Completed | 7/41 [02:22<11:26, 20.19s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  20% Completed | 8/41 [02:43<11:14, 20.45s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  22% Completed | 9/41 [03:06<11:22, 21.34s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  24% Completed | 10/41 [03:27<10:52, 21.05s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  27% Completed | 11/41 [03:46<10:19, 20.65s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  29% Completed | 12/41 [04:10<10:27, 21.64s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  32% Completed | 13/41 [04:32<10:09, 21.77s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  34% Completed | 14/41 [04:56<10:02, 22.30s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  37% Completed | 15/41 [05:17<09:29, 21.90s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  39% Completed | 16/41 [05:39<09:12, 22.11s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  41% Completed | 17/41 [06:00<08:41, 21.75s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  44% Completed | 18/41 [06:21<08:16, 21.58s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  46% Completed | 19/41 [06:43<07:54, 21.55s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  49% Completed | 20/41 [07:02<07:16, 20.77s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  51% Completed | 21/41 [07:22<06:49, 20.49s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  54% Completed | 22/41 [07:40<06:15, 19.75s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  56% Completed | 23/41 [08:05<06:24, 21.37s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  59% Completed | 24/41 [08:27<06:09, 21.71s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  61% Completed | 25/41 [08:48<05:40, 21.25s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  63% Completed | 26/41 [09:06<05:05, 20.35s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  66% Completed | 27/41 [09:27<04:48, 20.58s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  68% Completed | 28/41 [09:48<04:30, 20.78s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  71% Completed | 29/41 [10:07<04:01, 20.10s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  73% Completed | 30/41 [10:26<03:37, 19.75s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  76% Completed | 31/41 [10:46<03:18, 19.90s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  78% Completed | 32/41 [11:04<02:55, 19.48s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  80% Completed | 33/41 [11:24<02:35, 19.50s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  83% Completed | 34/41 [11:48<02:25, 20.76s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  85% Completed | 35/41 [12:07<02:02, 20.42s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  88% Completed | 36/41 [12:22<01:33, 18.64s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  90% Completed | 37/41 [12:38<01:11, 17.92s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  93% Completed | 38/41 [12:52<00:49, 16.66s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  95% Completed | 39/41 [13:12<00:35, 17.83s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards:  98% Completed | 40/41 [13:31<00:18, 18.21s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards: 100% Completed | 41/41 [13:50<00:00, 18.38s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m Loading safetensors checkpoint shards: 100% Completed | 41/41 [13:50<00:00, 20.26s/it]
[0;36m(Worker_TP0 pid=266775)[0;0m 
[0;36m(Worker_TP0 pid=266775)[0;0m INFO 12-05 18:31:35 [default_loader.py:308] Loading weights took 830.77 seconds
[0;36m(Worker_TP0 pid=266775)[0;0m INFO 12-05 18:31:36 [gpu_model_runner.py:3549] Model loading took 37.2151 GiB memory and 2037.309105 seconds
[0;36m(Worker_TP1 pid=266776)[0;0m WARNING 12-05 18:31:58 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/faker/JCM_Augmentation/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_RTX_A6000.json']
[0;36m(Worker_TP3 pid=266778)[0;0m WARNING 12-05 18:31:58 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/faker/JCM_Augmentation/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_RTX_A6000.json']
[0;36m(Worker_TP0 pid=266775)[0;0m WARNING 12-05 18:31:58 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/faker/JCM_Augmentation/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_RTX_A6000.json']
[0;36m(Worker_TP2 pid=266777)[0;0m WARNING 12-05 18:31:58 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/faker/JCM_Augmentation/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_RTX_A6000.json']
[0;36m(Worker_TP0 pid=266775)[0;0m INFO 12-05 18:32:09 [gpu_worker.py:359] Available KV cache memory: 5.23 GiB
[0;36m(EngineCore_DP0 pid=266633)[0;0m INFO 12-05 18:32:10 [kv_cache_utils.py:1286] GPU KV cache size: 114,240 tokens
[0;36m(EngineCore_DP0 pid=266633)[0;0m INFO 12-05 18:32:10 [kv_cache_utils.py:1291] Maximum concurrency for 2,048 tokens per request: 152.82x
[0;36m(EngineCore_DP0 pid=266633)[0;0m INFO 12-05 18:32:17 [core.py:254] init engine (profile, create kv cache, warmup model) took 40.83 seconds
[0;36m(EngineCore_DP0 pid=266633)[0;0m WARNING 12-05 18:32:19 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=266633)[0;0m INFO 12-05 18:32:19 [vllm.py:707] Cudagraph is disabled under eager mode
INFO 12-05 18:32:19 [llm.py:343] Supported tasks: ['generate']
Generating responses...
Adding requests:   0%|          | 0/3992 [00:00<?, ?it/s]Adding requests:   2%|â–         | 97/3992 [00:00<00:04, 967.51it/s]Adding requests:   7%|â–‹         | 260/3992 [00:00<00:02, 1352.20it/s]Adding requests:  11%|â–ˆ         | 422/3992 [00:00<00:02, 1474.14it/s]Adding requests:  15%|â–ˆâ–        | 584/3992 [00:00<00:02, 1530.23it/s]Adding requests:  19%|â–ˆâ–Š        | 743/3992 [00:00<00:02, 1550.45it/s]Adding requests:  23%|â–ˆâ–ˆâ–Ž       | 905/3992 [00:00<00:01, 1573.44it/s]Adding requests:  27%|â–ˆâ–ˆâ–‹       | 1086/3992 [00:00<00:01, 1648.66it/s]Adding requests:  32%|â–ˆâ–ˆâ–ˆâ–      | 1269/3992 [00:00<00:01, 1704.44it/s]Adding requests:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 1451/3992 [00:00<00:01, 1738.83it/s]Adding requests:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1629/3992 [00:01<00:01, 1750.60it/s]Adding requests:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1811/3992 [00:01<00:01, 1770.17it/s]Adding requests:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1994/3992 [00:01<00:01, 1787.80it/s]Adding requests:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2174/3992 [00:01<00:01, 1791.29it/s]Adding requests:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2354/3992 [00:01<00:00, 1791.07it/s]Adding requests:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2534/3992 [00:01<00:00, 1784.72it/s]Adding requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2715/3992 [00:01<00:00, 1792.13it/s]Adding requests:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2897/3992 [00:01<00:00, 1797.61it/s]Adding requests:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3081/3992 [00:01<00:00, 1808.61it/s]Adding requests:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3262/3992 [00:01<00:00, 1805.81it/s]Adding requests:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3443/3992 [00:02<00:00, 1790.79it/s]Adding requests:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3623/3992 [00:02<00:00, 1790.88it/s]Adding requests:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3803/3992 [00:02<00:00, 1792.33it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3985/3992 [00:02<00:00, 1799.17it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3992/3992 [00:02<00:00, 1724.55it/s]
Processed prompts:   0%|          | 0/3992 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][0;36m(EngineCore_DP0 pid=266633)[0;0m INFO 12-05 18:33:19 [shm_broadcast.py:501] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
Processed prompts:   0%|          | 1/3992 [01:43<114:44:25, 103.50s/it, est. speed input: 0.79 toks/s, output: 0.04 toks/s]Processed prompts:   0%|          | 2/3992 [01:43<47:33:00, 42.90s/it, est. speed input: 1.56 toks/s, output: 0.08 toks/s]  Processed prompts:   2%|â–         | 97/3992 [01:47<36:51,  1.76it/s, est. speed input: 71.44 toks/s, output: 3.61 toks/s] Processed prompts:   5%|â–         | 197/3992 [01:51<15:53,  3.98it/s, est. speed input: 140.01 toks/s, output: 7.08 toks/s]Processed prompts:   6%|â–Œ         | 242/3992 [01:53<12:06,  5.16it/s, est. speed input: 168.95 toks/s, output: 8.55 toks/s]Processed prompts:   6%|â–Œ         | 245/3992 [01:53<12:04,  5.17it/s, est. speed input: 170.32 toks/s, output: 8.61 toks/s]Processed prompts:   8%|â–Š         | 334/3992 [01:57<06:51,  8.88it/s, est. speed input: 225.44 toks/s, output: 11.40 toks/s]Processed prompts:  11%|â–ˆ         | 432/3992 [02:01<04:41, 12.66it/s, est. speed input: 283.52 toks/s, output: 14.29 toks/s]Processed prompts:  12%|â–ˆâ–        | 476/3992 [02:02<04:10, 14.05it/s, est. speed input: 307.72 toks/s, output: 15.49 toks/s]Processed prompts:  12%|â–ˆâ–        | 479/3992 [02:03<04:16, 13.69it/s, est. speed input: 308.50 toks/s, output: 15.53 toks/s]Processed prompts:  14%|â–ˆâ–        | 565/3992 [02:06<03:15, 17.49it/s, est. speed input: 354.41 toks/s, output: 17.83 toks/s]Processed prompts:  17%|â–ˆâ–‹        | 659/3992 [02:10<02:43, 20.42it/s, est. speed input: 402.11 toks/s, output: 20.23 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 701/3992 [02:12<02:38, 20.75it/s, est. speed input: 421.68 toks/s, output: 21.21 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 704/3992 [02:12<02:46, 19.78it/s, est. speed input: 421.95 toks/s, output: 21.22 toks/s]Processed prompts:  20%|â–ˆâ–‰        | 785/3992 [02:15<02:26, 21.88it/s, est. speed input: 459.35 toks/s, output: 23.10 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 873/3992 [02:19<02:12, 23.46it/s, est. speed input: 498.80 toks/s, output: 25.07 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 912/3992 [02:21<02:13, 23.10it/s, est. speed input: 514.93 toks/s, output: 25.86 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 915/3992 [02:21<02:21, 21.80it/s, est. speed input: 514.84 toks/s, output: 26.19 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 999/3992 [02:24<02:07, 23.43it/s, est. speed input: 549.58 toks/s, output: 28.58 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 1084/3992 [02:28<01:59, 24.28it/s, est. speed input: 583.07 toks/s, output: 30.56 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 1127/3992 [02:30<02:00, 23.85it/s, est. speed input: 598.34 toks/s, output: 31.56 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 1130/3992 [02:30<02:07, 22.46it/s, est. speed input: 597.84 toks/s, output: 31.86 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 1213/3992 [02:33<01:57, 23.72it/s, est. speed input: 627.74 toks/s, output: 33.65 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1298/3992 [02:37<01:50, 24.36it/s, est. speed input: 658.01 toks/s, output: 35.30 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1342/3992 [02:39<01:50, 23.92it/s, est. speed input: 671.85 toks/s, output: 36.08 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1345/3992 [02:39<01:58, 22.27it/s, est. speed input: 671.08 toks/s, output: 36.23 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1427/3992 [02:42<01:48, 23.70it/s, est. speed input: 698.12 toks/s, output: 37.63 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1512/3992 [02:46<01:40, 24.59it/s, est. speed input: 726.37 toks/s, output: 39.32 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1558/3992 [02:48<01:40, 24.19it/s, est. speed input: 739.76 toks/s, output: 40.04 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1561/3992 [02:48<01:47, 22.54it/s, est. speed input: 738.81 toks/s, output: 40.17 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1640/3992 [02:51<01:38, 23.76it/s, est. speed input: 762.14 toks/s, output: 41.66 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1724/3992 [02:55<01:32, 24.63it/s, est. speed input: 786.40 toks/s, output: 43.27 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1772/3992 [02:57<01:31, 24.28it/s, est. speed input: 798.76 toks/s, output: 44.07 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1775/3992 [02:57<01:38, 22.62it/s, est. speed input: 797.59 toks/s, output: 44.18 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1857/3992 [03:00<01:29, 23.73it/s, est. speed input: 819.10 toks/s, output: 45.55 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1944/3992 [03:04<01:23, 24.60it/s, est. speed input: 841.60 toks/s, output: 47.05 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1990/3992 [03:06<01:22, 24.15it/s, est. speed input: 852.25 toks/s, output: 47.53 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1993/3992 [03:06<01:29, 22.43it/s, est. speed input: 850.94 toks/s, output: 47.62 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2076/3992 [03:10<01:20, 23.72it/s, est. speed input: 871.46 toks/s, output: 48.97 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2163/3992 [03:13<01:13, 24.73it/s, est. speed input: 892.45 toks/s, output: 50.02 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2208/3992 [03:15<01:13, 24.31it/s, est. speed input: 902.14 toks/s, output: 50.44 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2211/3992 [03:15<01:17, 22.90it/s, est. speed input: 901.06 toks/s, output: 50.62 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2295/3992 [03:19<01:10, 23.99it/s, est. speed input: 919.85 toks/s, output: 51.55 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2380/3992 [03:22<01:05, 24.79it/s, est. speed input: 938.17 toks/s, output: 52.63 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2420/3992 [03:24<01:05, 24.08it/s, est. speed input: 945.25 toks/s, output: 52.94 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2423/3992 [03:24<01:09, 22.65it/s, est. speed input: 944.11 toks/s, output: 53.11 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2510/3992 [03:28<01:08, 21.49it/s, est. speed input: 958.03 toks/s, output: 54.15 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2593/3992 [03:32<01:00, 22.99it/s, est. speed input: 974.57 toks/s, output: 55.27 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2633/3992 [03:34<00:59, 22.76it/s, est. speed input: 981.38 toks/s, output: 55.70 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2636/3992 [03:34<01:03, 21.50it/s, est. speed input: 980.27 toks/s, output: 55.78 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2723/3992 [03:37<00:54, 23.22it/s, est. speed input: 996.82 toks/s, output: 56.57 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2803/3992 [03:41<00:49, 24.19it/s, est. speed input: 1011.84 toks/s, output: 57.45 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2843/3992 [03:42<00:48, 23.61it/s, est. speed input: 1017.72 toks/s, output: 57.76 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2846/3992 [03:43<00:51, 22.21it/s, est. speed input: 1016.70 toks/s, output: 57.91 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2938/3992 [03:46<00:44, 23.71it/s, est. speed input: 1032.96 toks/s, output: 58.82 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3018/3992 [03:50<00:40, 24.32it/s, est. speed input: 1046.08 toks/s, output: 59.54 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3059/3992 [03:51<00:39, 23.76it/s, est. speed input: 1051.82 toks/s, output: 59.84 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3062/3992 [03:52<00:42, 22.12it/s, est. speed input: 1050.40 toks/s, output: 59.82 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3150/3992 [03:55<00:35, 23.74it/s, est. speed input: 1065.13 toks/s, output: 60.51 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3234/3992 [03:59<00:31, 24.29it/s, est. speed input: 1078.31 toks/s, output: 61.47 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3277/3992 [04:01<00:29, 23.84it/s, est. speed input: 1083.94 toks/s, output: 61.96 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3280/3992 [04:01<00:32, 22.13it/s, est. speed input: 1082.46 toks/s, output: 61.94 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3373/3992 [04:05<00:25, 24.01it/s, est. speed input: 1097.05 toks/s, output: 62.88 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3460/3992 [04:08<00:21, 24.60it/s, est. speed input: 1110.71 toks/s, output: 63.87 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3506/3992 [04:10<00:20, 24.16it/s, est. speed input: 1116.36 toks/s, output: 64.22 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3509/3992 [04:11<00:21, 22.42it/s, est. speed input: 1114.71 toks/s, output: 64.18 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3601/3992 [04:14<00:16, 24.14it/s, est. speed input: 1128.26 toks/s, output: 64.87 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3688/3992 [04:18<00:12, 24.79it/s, est. speed input: 1141.02 toks/s, output: 65.75 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3735/3992 [04:20<00:10, 24.42it/s, est. speed input: 1146.74 toks/s, output: 66.03 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3738/3992 [04:20<00:11, 22.83it/s, est. speed input: 1145.41 toks/s, output: 65.94 toks/s]Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3824/3992 [04:21<00:04, 39.07it/s, est. speed input: 1170.24 toks/s, output: 67.22 toks/s]Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3912/3992 [04:21<00:01, 61.07it/s, est. speed input: 1196.38 toks/s, output: 68.57 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3959/3992 [04:21<00:00, 67.12it/s, est. speed input: 1208.37 toks/s, output: 69.17 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3974/3992 [04:23<00:00, 44.36it/s, est. speed input: 1206.26 toks/s, output: 69.87 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3985/3992 [04:24<00:00, 32.77it/s, est. speed input: 1203.91 toks/s, output: 70.39 toks/s][0;36m(Worker_TP3 pid=266778)[0;0m [rank3]:W1205 18:36:51.159000 266778 torch/_dynamo/convert_frame.py:1358] [1/8] torch._dynamo hit config.recompile_limit (8)
[0;36m(Worker_TP3 pid=266778)[0;0m [rank3]:W1205 18:36:51.159000 266778 torch/_dynamo/convert_frame.py:1358] [1/8]    function: 'forward_static' (/home/faker/JCM_Augmentation/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py:274)
[0;36m(Worker_TP3 pid=266778)[0;0m [rank3]:W1205 18:36:51.159000 266778 torch/_dynamo/convert_frame.py:1358] [1/8]    last reason: 1/7: tensor 'x' rank mismatch. expected 2, actual 3
[0;36m(Worker_TP3 pid=266778)[0;0m [rank3]:W1205 18:36:51.159000 266778 torch/_dynamo/convert_frame.py:1358] [1/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[0;36m(Worker_TP3 pid=266778)[0;0m [rank3]:W1205 18:36:51.159000 266778 torch/_dynamo/convert_frame.py:1358] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html
[0;36m(Worker_TP2 pid=266777)[0;0m [rank2]:W1205 18:36:51.275000 266777 torch/_dynamo/convert_frame.py:1358] [1/8] torch._dynamo hit config.recompile_limit (8)
[0;36m(Worker_TP2 pid=266777)[0;0m [rank2]:W1205 18:36:51.275000 266777 torch/_dynamo/convert_frame.py:1358] [1/8]    function: 'forward_static' (/home/faker/JCM_Augmentation/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py:274)
[0;36m(Worker_TP2 pid=266777)[0;0m [rank2]:W1205 18:36:51.275000 266777 torch/_dynamo/convert_frame.py:1358] [1/8]    last reason: 1/7: tensor 'x' rank mismatch. expected 2, actual 3
[0;36m(Worker_TP2 pid=266777)[0;0m [rank2]:W1205 18:36:51.275000 266777 torch/_dynamo/convert_frame.py:1358] [1/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[0;36m(Worker_TP2 pid=266777)[0;0m [rank2]:W1205 18:36:51.275000 266777 torch/_dynamo/convert_frame.py:1358] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html
[0;36m(Worker_TP1 pid=266776)[0;0m [rank1]:W1205 18:36:51.279000 266776 torch/_dynamo/convert_frame.py:1358] [1/8] torch._dynamo hit config.recompile_limit (8)
[0;36m(Worker_TP1 pid=266776)[0;0m [rank1]:W1205 18:36:51.279000 266776 torch/_dynamo/convert_frame.py:1358] [1/8]    function: 'forward_static' (/home/faker/JCM_Augmentation/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py:274)
[0;36m(Worker_TP1 pid=266776)[0;0m [rank1]:W1205 18:36:51.279000 266776 torch/_dynamo/convert_frame.py:1358] [1/8]    last reason: 1/7: tensor 'x' rank mismatch. expected 2, actual 3
[0;36m(Worker_TP1 pid=266776)[0;0m [rank1]:W1205 18:36:51.279000 266776 torch/_dynamo/convert_frame.py:1358] [1/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[0;36m(Worker_TP1 pid=266776)[0;0m [rank1]:W1205 18:36:51.279000 266776 torch/_dynamo/convert_frame.py:1358] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html
[0;36m(Worker_TP0 pid=266775)[0;0m [rank0]:W1205 18:36:51.313000 266775 torch/_dynamo/convert_frame.py:1358] [1/8] torch._dynamo hit config.recompile_limit (8)
[0;36m(Worker_TP0 pid=266775)[0;0m [rank0]:W1205 18:36:51.313000 266775 torch/_dynamo/convert_frame.py:1358] [1/8]    function: 'forward_static' (/home/faker/JCM_Augmentation/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/layernorm.py:274)
[0;36m(Worker_TP0 pid=266775)[0;0m [rank0]:W1205 18:36:51.313000 266775 torch/_dynamo/convert_frame.py:1358] [1/8]    last reason: 1/7: tensor 'x' rank mismatch. expected 2, actual 3
[0;36m(Worker_TP0 pid=266775)[0;0m [rank0]:W1205 18:36:51.313000 266775 torch/_dynamo/convert_frame.py:1358] [1/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[0;36m(Worker_TP0 pid=266775)[0;0m [rank0]:W1205 18:36:51.313000 266775 torch/_dynamo/convert_frame.py:1358] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3992/3992 [04:29<00:00, 32.77it/s, est. speed input: 1182.44 toks/s, output: 69.54 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3992/3992 [04:29<00:00, 14.80it/s, est. speed input: 1182.44 toks/s, output: 69.54 toks/s]
[0;36m(Worker_TP3 pid=266778)[0;0m INFO 12-05 18:36:51 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP0 pid=266775)[0;0m INFO 12-05 18:36:51 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP1 pid=266776)[0;0m INFO 12-05 18:36:51 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP2 pid=266777)[0;0m INFO 12-05 18:36:51 [multiproc_executor.py:709] Parent process exited, terminating worker
Model: Qwen/Qwen3-Next-80B-A3B-Instruct
Accuracy: 0.9108
F1 Score: 0.9060
Detailed results saved to /home/faker/JCM_Augmentation/data/03_model_selection/Qwen3-Next-80B-A3B-Instruct/prediction_results.csv
