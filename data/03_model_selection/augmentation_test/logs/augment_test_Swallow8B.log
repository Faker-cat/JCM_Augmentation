nohup: ignoring input
Loading data from /home/faker/JCM_Augmentation/data/03_model_selection/ambiguous_sentences_random_10.csv...
Data loaded successfully.
Initializing model: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5...
INFO 12-07 13:48:01 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'max_model_len': 2048, 'tensor_parallel_size': 2, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
WARNING 12-07 13:48:01 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
INFO 12-07 13:48:02 [model.py:637] Resolved architecture: LlamaForCausalLM
INFO 12-07 13:48:02 [model.py:1750] Using max model len 2048
INFO 12-07 13:48:02 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-07 13:48:02 [vllm.py:601] Enforce eager set, overriding optimization level to -O0
INFO 12-07 13:48:02 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:04 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5', speculative_config=None, tokenizer='tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=166225)[0;0m WARNING 12-07 13:48:04 [multiproc_executor.py:880] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:08 [parallel_state.py:1200] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59005 backend=nccl
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:08 [parallel_state.py:1200] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59005 backend=nccl
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:08 [pynccl.py:111] vLLM is using nccl==2.27.5
[0;36m(EngineCore_DP0 pid=166225)[0;0m WARNING 12-07 13:48:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=166225)[0;0m WARNING 12-07 13:48:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:09 [parallel_state.py:1408] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:09 [parallel_state.py:1408] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m INFO 12-07 13:48:09 [gpu_model_runner.py:3467] Starting to load model tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5...
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m INFO 12-07 13:48:10 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP1 pid=166233)[0;0m INFO 12-07 13:48:10 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.24it/s]
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.32it/s]
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.00s/it]
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.11it/s]
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m 
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m INFO 12-07 13:48:15 [default_loader.py:308] Loading weights took 3.69 seconds
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m INFO 12-07 13:48:16 [gpu_model_runner.py:3549] Model loading took 7.5123 GiB memory and 5.203237 seconds
[0;36m(EngineCore_DP0 pid=166225)[0;0m [0;36m(Worker_TP0 pid=166231)[0;0m INFO 12-07 13:48:28 [gpu_worker.py:359] Available KV cache memory: 33.76 GiB
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:29 [kv_cache_utils.py:1286] GPU KV cache size: 553,136 tokens
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:29 [kv_cache_utils.py:1291] Maximum concurrency for 2,048 tokens per request: 270.09x
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:29 [core.py:254] init engine (profile, create kv cache, warmup model) took 13.07 seconds
[0;36m(EngineCore_DP0 pid=166225)[0;0m WARNING 12-07 13:48:32 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=166225)[0;0m INFO 12-07 13:48:32 [vllm.py:707] Cudagraph is disabled under eager mode
INFO 12-07 13:48:32 [llm.py:343] Supported tasks: ['generate']
Generating responses...
Adding requests:   0%|          | 0/11 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 280.85it/s]
Processed prompts:   0%|          | 0/11 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   9%|â–‰         | 1/11 [00:01<00:11,  1.17s/it, est. speed input: 227.47 toks/s, output: 20.60 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 3/11 [00:01<00:03,  2.29it/s, est. speed input: 516.54 toks/s, output: 54.92 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 4/11 [00:01<00:03,  2.24it/s, est. speed input: 527.72 toks/s, output: 65.65 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6/11 [00:02<00:01,  2.56it/s, est. speed input: 596.35 toks/s, output: 92.70 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 7/11 [00:03<00:02,  2.00it/s, est. speed input: 534.40 toks/s, output: 96.80 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8/11 [00:04<00:01,  1.80it/s, est. speed input: 509.36 toks/s, output: 106.71 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9/11 [00:04<00:00,  2.14it/s, est. speed input: 541.04 toks/s, output: 127.10 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [00:04<00:00,  2.31it/s, est. speed input: 556.09 toks/s, output: 144.33 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.75it/s, est. speed input: 588.62 toks/s, output: 164.51 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.75it/s, est. speed input: 588.62 toks/s, output: 164.51 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.22it/s, est. speed input: 588.62 toks/s, output: 164.51 toks/s]
Results saved to data/03_model_selection/augmentation_test/Llama-3.1-Swallow-8B-Instruct-v0.5_augmented_results_10samples.csv
